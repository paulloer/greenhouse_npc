{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Using 0 workers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.use(\"pgf\")\n",
    "mpl.rcParams.update(\n",
    "{\n",
    "    \"pgf.texsystem\":   \"pdflatex\", # or any other engine you want to use\n",
    "    \"text.usetex\":     True,       # use TeX for all texts\n",
    "    \"font.family\":     \"serif\",\n",
    "    \"font.serif\":      [],         # empty entries should cause the usage of the document fonts\n",
    "    \"font.sans-serif\": [],\n",
    "    \"font.monospace\":  [],\n",
    "    \"font.size\":       10,         # control font sizes of different elements\n",
    "    \"axes.labelsize\":  10,\n",
    "    \"legend.fontsize\": 9,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "})\n",
    "\n",
    "from utilities import SimpleTemporalFusionTransformer, GreenhouseDatasetHandler, train, test, plot_loss, learn, transfer_learning_with_noise, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 128\n",
    "attention_head_size = 8\n",
    "num_epochs = 200\n",
    "batch_size = 16\n",
    "weight_decay = 1e-3\n",
    "num_lstm_layers = 1\n",
    "dropout_prob = 0.5\n",
    "stride = 10\n",
    "\n",
    "hours_m = 1\n",
    "hours_p = 12\n",
    "N_m = hours_m * int(120/stride)  # Past timesteps\n",
    "N_p = hours_p * int(120/stride)  # Future timesteps\n",
    "\n",
    "#config_string = f\"{N_m}_Nm_{N_p}_Np_lr_{learning_rate}_hs_{hidden_size}_ahs_{attention_head_size}_nll_{num_lstm_layers}_bs_{batch_size}_wd_{weight_decay}_do_{dropout_prob}_e_{num_epochs}_stride_{stride}_sgd\"\n",
    "config_string = f\"{N_m}_Nm_{N_p}_Np_lr_{learning_rate}_hs_{hidden_size}_ahs_{attention_head_size}_nll_{num_lstm_layers}_bs_{batch_size}_do_{dropout_prob}_e_{num_epochs}_stride_{stride}\"\n",
    "save_dir = os.getcwd() + f\"/transformer_no_tanh_with_past_{config_string}\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess datasets\n",
    "data_N25 = pd.read_csv('GH_Data_2025-11-19_2025-11-21.csv', delimiter=',')\n",
    "data_N25_test = pd.read_csv('GH_Data_2025-11-25_2025-11-26.csv', delimiter=',')\n",
    "\n",
    "state_features = ['Temperature_inside', 'Humidity_inside']\n",
    "control_features_W24 = ['Vent_S1_Roof_1', 'Vent_S1_Roof_2', 'Vent_S1_Roof_3', \n",
    "                                'Vent_S1_Side_N', 'Vent_S1_Side_NW', 'Vent_S1_Side_S', 'Vent_S1_Side_SW', \n",
    "                                'Vent_S2_Roof_1', 'Vent_S2_Roof_2', 'Vent_S2_Roof_3',\n",
    "                                'Vent_S2_Side_E', 'Vent_S2_Side_N', 'Vent_S2_Side_S']\n",
    "control_features_W24 = [control_features_W24[4]]\n",
    "disturbance_features = ['Temperature_outside', 'Humidity_outside', 'Radiation_inside', 'Radiation_outside', 'Wind_speed_outside']\n",
    "\n",
    "split_index_train = int(22/36 * len(data_N25))\n",
    "split_index_val = int(36/36 * len(data_N25))\n",
    "# split_index_test = int(4/8 * len(data_J25))\n",
    "dataset_N25 = GreenhouseDatasetHandler(data=data_N25, \n",
    "                                       train_data=data_N25.iloc[:split_index_train],\n",
    "                                       val_data=data_N25.iloc[split_index_train:split_index_val], \n",
    "                                    #    test_data=data_N25.iloc[split_index_val:], \n",
    "                                       test_data=data_N25_test,\n",
    "                                       state_features=state_features, \n",
    "                                       control_features=control_features_W24,  # only one control\n",
    "                                       disturbance_features=disturbance_features, \n",
    "                                       seq_len=N_m, pred_len=N_p, stride=stride, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "model_A25_to_J25 = SimpleTemporalFusionTransformer(\n",
    "    len(state_features), len(control_features_W24), len(disturbance_features),\n",
    "    hidden_dim=hidden_size,\n",
    "    num_heads=attention_head_size,\n",
    "    num_layers=num_lstm_layers, \n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "model_N25 = SimpleTemporalFusionTransformer(\n",
    "    len(state_features), len(control_features_W24), len(disturbance_features),\n",
    "    hidden_dim=hidden_size,\n",
    "    num_heads=attention_head_size,\n",
    "    num_layers=num_lstm_layers, \n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn from 2-day Dataset (November 2025, GH with 13 vents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.759663, Validation Loss: 1.141201, LR: 9.955000e-04\n",
      "Epoch [6/200], Training Loss: 0.394152, Validation Loss: 1.115318, LR: 9.730000e-04\n",
      "Epoch [11/200], Training Loss: 0.237200, Validation Loss: 1.082060, LR: 9.505000e-04\n",
      "Epoch [16/200], Training Loss: 0.186924, Validation Loss: 1.004755, LR: 9.280000e-04\n",
      "Epoch [21/200], Training Loss: 0.160558, Validation Loss: 0.907565, LR: 9.055000e-04\n",
      "Epoch [26/200], Training Loss: 0.140607, Validation Loss: 0.812376, LR: 8.830000e-04\n",
      "Epoch [31/200], Training Loss: 0.124070, Validation Loss: 0.727005, LR: 8.605000e-04\n",
      "Epoch [36/200], Training Loss: 0.110974, Validation Loss: 0.653016, LR: 8.380000e-04\n",
      "Epoch [41/200], Training Loss: 0.100375, Validation Loss: 0.589938, LR: 8.155000e-04\n",
      "Epoch [46/200], Training Loss: 0.092009, Validation Loss: 0.536822, LR: 7.930000e-04\n",
      "Epoch [51/200], Training Loss: 0.085477, Validation Loss: 0.492234, LR: 7.705000e-04\n",
      "Epoch [56/200], Training Loss: 0.079751, Validation Loss: 0.455043, LR: 7.480000e-04\n",
      "Epoch [61/200], Training Loss: 0.075776, Validation Loss: 0.423894, LR: 7.255000e-04\n",
      "Epoch [66/200], Training Loss: 0.071938, Validation Loss: 0.397877, LR: 7.030000e-04\n",
      "Epoch [71/200], Training Loss: 0.069107, Validation Loss: 0.375957, LR: 6.805000e-04\n",
      "Epoch [76/200], Training Loss: 0.066453, Validation Loss: 0.357438, LR: 6.580000e-04\n",
      "Epoch [81/200], Training Loss: 0.064114, Validation Loss: 0.341694, LR: 6.355000e-04\n",
      "Epoch [86/200], Training Loss: 0.062068, Validation Loss: 0.328309, LR: 6.130000e-04\n",
      "Epoch [91/200], Training Loss: 0.060281, Validation Loss: 0.316808, LR: 5.905000e-04\n",
      "Epoch [96/200], Training Loss: 0.058896, Validation Loss: 0.306861, LR: 5.680000e-04\n",
      "Epoch [101/200], Training Loss: 0.057301, Validation Loss: 0.298141, LR: 5.455000e-04\n",
      "Epoch [106/200], Training Loss: 0.055992, Validation Loss: 0.290522, LR: 5.230000e-04\n",
      "Epoch [111/200], Training Loss: 0.054788, Validation Loss: 0.283907, LR: 5.005000e-04\n",
      "Epoch [116/200], Training Loss: 0.053656, Validation Loss: 0.278032, LR: 4.780000e-04\n",
      "Epoch [121/200], Training Loss: 0.052696, Validation Loss: 0.272849, LR: 4.555000e-04\n",
      "Epoch [126/200], Training Loss: 0.051902, Validation Loss: 0.268168, LR: 4.330000e-04\n",
      "Epoch [131/200], Training Loss: 0.051033, Validation Loss: 0.264090, LR: 4.105000e-04\n",
      "Epoch [136/200], Training Loss: 0.050326, Validation Loss: 0.260375, LR: 3.880000e-04\n",
      "Epoch [141/200], Training Loss: 0.049637, Validation Loss: 0.257047, LR: 3.655000e-04\n",
      "Epoch [146/200], Training Loss: 0.049002, Validation Loss: 0.254082, LR: 3.430000e-04\n",
      "Epoch [151/200], Training Loss: 0.048405, Validation Loss: 0.251397, LR: 3.205000e-04\n",
      "Epoch [156/200], Training Loss: 0.047892, Validation Loss: 0.249020, LR: 2.980000e-04\n",
      "Epoch [161/200], Training Loss: 0.047369, Validation Loss: 0.246877, LR: 2.755000e-04\n",
      "Epoch [166/200], Training Loss: 0.046993, Validation Loss: 0.244965, LR: 2.530000e-04\n",
      "Epoch [171/200], Training Loss: 0.046557, Validation Loss: 0.243262, LR: 2.305000e-04\n",
      "Epoch [176/200], Training Loss: 0.046244, Validation Loss: 0.241754, LR: 2.080000e-04\n",
      "Epoch [181/200], Training Loss: 0.045971, Validation Loss: 0.240435, LR: 1.855000e-04\n",
      "Epoch [186/200], Training Loss: 0.045966, Validation Loss: 0.239288, LR: 1.630000e-04\n",
      "Epoch [191/200], Training Loss: 0.045526, Validation Loss: 0.238305, LR: 1.405000e-04\n",
      "Epoch [196/200], Training Loss: 0.045301, Validation Loss: 0.237468, LR: 1.180000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'serif' not found because none of the following families were found: \n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: \n",
      "findfont: Font family ['monospace'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Generic family 'monospace' not found because none of the following families were found: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/200], Training Loss: 0.045279, Validation Loss: 0.236919, LR: 1.000000e-04\n",
      "Saving model_N25_0.001lr_200e state dict to /Users/paulloer/Desktop/online_optimization_and_learning/transformer_no_tanh_with_past_12_Nm_144_Np_lr_0.001_hs_128_ahs_8_nll_1_bs_16_do_0.5_e_200_stride_10/model_N25_0.001lr_200e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 3012/3012 [00:14<00:00, 208.74it/s]\n",
      "/Users/paulloer/Desktop/online_optimization_and_learning/utilities.py:274: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[0].legend()\n",
      "/Users/paulloer/Desktop/online_optimization_and_learning/utilities.py:278: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  axs[1].legend()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 3012\n",
      "RMS Temperature Error of 2.1283 with standard deviation 1.9162\n",
      "RMS Humidity Error of 23.1871 with standard deviation 11.1744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.128341, 23.18712, 1.9162217, 11.174354)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "model_N25 = SimpleTemporalFusionTransformer(\n",
    "    len(state_features), len(control_features_W24), len(disturbance_features),\n",
    "    hidden_dim=hidden_size,\n",
    "    num_heads=attention_head_size,\n",
    "    num_layers=num_lstm_layers, \n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "model_N25_path = f\"{save_dir}/model_N25_{learning_rate}lr_{num_epochs}e.pth\"\n",
    "learn(model_N25, f'model_N25_{learning_rate}lr_{num_epochs}e', model_N25_path, dataset_N25, num_epochs, learning_rate, weight_decay, save_dir, config_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: Add noise to model A25toJ25 before training on dataset N25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "noise = 0.01\n",
    "model_A25_to_J25_path = f\"{save_dir}/model_transfer_A25_to_J25_0.01n_0.001lr_200e.pth\"\n",
    "model_name = f'model_transfer_A25_to_J25_to_N25_{noise}n_{learning_rate}lr_{num_epochs}e'\n",
    "model_A25_to_J25_to_N25_path = f\"{save_dir}/{model_name}.pth\"\n",
    "\n",
    "model_transfer_A25_to_J25 = SimpleTemporalFusionTransformer(\n",
    "    len(state_features), len(control_features_W24), len(disturbance_features),\n",
    "    hidden_dim=hidden_size,\n",
    "    num_heads=attention_head_size,\n",
    "    num_layers=num_lstm_layers, \n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "transfer_learning_with_noise(model=model_N25, \n",
    "                             model_name=model_name, \n",
    "                             model_path_source=model_A25_to_J25_path, \n",
    "                             model_path_target=model_A25_to_J25_to_N25_path, \n",
    "                             dataset=dataset_N25, \n",
    "                             num_epochs=num_epochs, learning_rate=learning_rate, weight_decay=weight_decay, save_dir=save_dir, config_string=config_string, noise=noise\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model transfer A25 to J25 on dataset A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer_A25_to_J25.load_state_dict(torch.load(model_A25_to_J25_path, map_location=torch.device(device.type), weights_only=True))\n",
    "test(model_transfer_A25_to_J25, dataset_A25, save_dir, f'model_transfer_A25_to_J25_on_A25_{learning_rate}lr_{num_epochs}e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: Add noise to model W20 before training on two days from dataset W24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "noise = 0.01\n",
    "model_name = f'model_transfer_W20_to_W24_2d_path_{noise}noise_2d_{learning_rate}lr_{num_epochs}e'\n",
    "model_transfer_W20_to_W24_2d_path = f\"{save_dir}/{model_name}.pth\"\n",
    "\n",
    "model_transfer_W20_to_W24_2d = SimpleTemporalFusionTransformer(\n",
    "    len(state_features), len(control_features_W20), len(disturbance_features),\n",
    "    hidden_dim=hidden_size,\n",
    "    num_heads=attention_head_size,\n",
    "    num_layers=num_lstm_layers, \n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "split_index_36 = int(36/48 * len(data_W24))\n",
    "split_index_80 = int(38/48 * len(data_W24))\n",
    "split_index_90 = int(43/48 * len(data_W24))\n",
    "dataset_W24_2d = GreenhouseDatasetHandler(data=data_W24, \n",
    "                                       train_data=data_W24.iloc[split_index_36:split_index_80],  # pd.concat([data_W24.iloc[:split_index_45], data_W24.iloc[split_index_50:split_index_95]]), \n",
    "                                       val_data=data_W24.iloc[split_index_80:split_index_90], \n",
    "                                       test_data=data_W24.iloc[split_index_90:], \n",
    "                                       state_features=state_features, \n",
    "                                       control_features=control_features_W24,  # only one control\n",
    "                                       disturbance_features=disturbance_features, \n",
    "                                       seq_len=N_m, pred_len=N_p, stride=stride, batch_size=batch_size)\n",
    "\n",
    "transfer_learning_with_noise(model_transfer_W20_to_W24_2d, model_name, model_W20_path, model_transfer_W20_to_W24_2d_path, dataset_W24_2d, num_epochs, learning_rate, weight_decay, save_dir, config_string, noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model transfer W20 to W24 2d directly on dataset A25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer_W20_to_W24_2d.load_state_dict(torch.load(model_transfer_W20_to_W24_2d_path, map_location=torch.device(device.type), weights_only=True))\n",
    "test(model_transfer_W20_to_W24_2d, dataset_A25, save_dir, f'model_transfer_W20_to_W24_2d_on_A25')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
